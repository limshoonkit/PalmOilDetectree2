{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71fa8530",
   "metadata": {},
   "source": [
    "## Install the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afe67d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%pip3` not found.\n"
     ]
    }
   ],
   "source": [
    "# Change to your version of CUDA\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64fd83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdal==3.4.1 in /home/ubuntu/uosm-cirg/PalmOilDetectree2/.venv/lib/python3.10/site-packages (3.4.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%pip3` not found.\n"
     ]
    }
   ],
   "source": [
    "# If gdal not installed, run: sudo apt install libgdal-dev gdal-bin\n",
    "import subprocess\n",
    "\n",
    "# Get the version of GDAL installed on the system via gdal-config\n",
    "gdal_version = subprocess.check_output([\"gdal-config\", \"--version\"]).decode(\"utf-8\").strip()\n",
    "\n",
    "# Use pip to install the corresponding GDAL version\n",
    "%pip install gdal=={gdal_version}\n",
    "\n",
    "# Downgrade numpy to < 2.0.0\n",
    "%pip install numpy==1.26.4\n",
    "\n",
    "# Get cv2\n",
    "%pip install opencv-python\n",
    "\n",
    "# Jupyter specifics\n",
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37d5a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tested on version 2.0.1 for detectree2\n",
    "%pip install git+https://github.com/PatBall1/detectree2.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4119ba87",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Important!\n",
    "1. Remove eval/ folder if dataset changed\n",
    "2. Depending on base model, need sufficient GPU VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b04be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test package imports, do not proceed if there are errors\n",
    "import os\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "from detectron2.data.catalog import DatasetCatalog\n",
    "from detectree2.preprocessing.tiling import tile_data, to_traintest_folders\n",
    "from detectree2.models.train import register_train_data, MyTrainer, setup_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d16264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base path for all sites\n",
    "base_path = \"../dataset\"\n",
    "\n",
    "# Parameters (change these as needed)\n",
    "buffer = 10\n",
    "tile_width = 15\n",
    "tile_height = 15\n",
    "threshold = 0.5\n",
    "appends = f\"{tile_width}_{buffer}_{threshold}\"\n",
    "test_frac = 0.15\n",
    "folds = 5\n",
    "\n",
    "# Function to get site name from folder path\n",
    "def get_site_name(folder_path):\n",
    "    # Extract the site name from the folder path\n",
    "    return os.path.basename(folder_path)\n",
    "\n",
    "# Get all immediate subdirectories of the base path (all sites)\n",
    "site_folders = [f for f in os.listdir(base_path) \n",
    "                if os.path.isdir(os.path.join(base_path, f))]\n",
    "\n",
    "print(f\"Found {len(site_folders)} site folders: {site_folders}\")\n",
    "\n",
    "# List to store all registered dataset names\n",
    "all_train_datasets = []\n",
    "all_val_datasets = []\n",
    "\n",
    "# Process each site folder\n",
    "for site_folder in site_folders:\n",
    "    site_path = os.path.join(base_path, site_folder)\n",
    "    site_name = get_site_name(site_path)\n",
    "    \n",
    "    # Construct paths for this site\n",
    "    img_path = os.path.join(site_path, \"rgb\", f\"{site_folder}.tif\")\n",
    "    crown_path = os.path.join(site_path, \"crowns\", f\"{site_folder}.gpkg\")\n",
    "    out_dir = os.path.join(site_path, f\"tiles_{appends}/\")\n",
    "    \n",
    "    print(f\"Processing site: {site_folder}\")\n",
    "    \n",
    "    # Ensure the image and crown files exist\n",
    "    if not os.path.exists(img_path):\n",
    "        print(f\"  Warning: Image file not found at {img_path}\")\n",
    "        continue\n",
    "    if not os.path.exists(crown_path):\n",
    "        print(f\"  Warning: Crown file not found at {crown_path}\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Read in the tiff file for CRS info\n",
    "        data = rasterio.open(img_path)\n",
    "        \n",
    "        # Read in crowns and ensure matching CRS\n",
    "        crowns = gpd.read_file(crown_path)\n",
    "        crowns = crowns.to_crs(data.crs.data)  # Making sure CRS match\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        \n",
    "        # Run tile_data function\n",
    "        print(f\"  Running tile_data for {site_folder}...\")\n",
    "        tile_data(img_path, out_dir, buffer, tile_width, tile_height, crowns, threshold, mode=\"rgb\")\n",
    "        \n",
    "        # Run to_traintest_folders function\n",
    "        print(f\"  Running to_traintest_folders for {site_folder}...\")\n",
    "        to_traintest_folders(out_dir, out_dir, test_frac=test_frac, strict=False, folds=folds)\n",
    "        \n",
    "        # Register the dataset\n",
    "        dataset_name = f\"oilpalm_msia_{site_name}\"\n",
    "        train_location = os.path.join(site_path, f\"tiles_{appends}/train\")\n",
    "        \n",
    "        print(f\"  Registering dataset: {dataset_name}\")\n",
    "        register_train_data(train_location, dataset_name, val_fold=folds)\n",
    "        \n",
    "        # Add to our lists of registered datasets\n",
    "        all_train_datasets.append(f\"{dataset_name}_train\")\n",
    "        all_val_datasets.append(f\"{dataset_name}_val\")\n",
    "        \n",
    "        print(f\"  Completed processing {site_folder}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing {site_folder}: {str(e)}\")\n",
    "\n",
    "# Convert lists to tuples for Detectron2\n",
    "all_train_datasets = tuple(all_train_datasets)\n",
    "all_val_datasets = tuple(all_val_datasets)\n",
    "\n",
    "print(\"All sites processed.\")\n",
    "print(\"Available datasets:\", DatasetCatalog.list())\n",
    "print(\"Registered train datasets:\", all_train_datasets)\n",
    "print(\"Registered validation datasets:\", all_val_datasets)\n",
    "\n",
    "# Setup base model from facebookresearch/detectron2\n",
    "# https://github.com/facebookresearch/detectron2/blob/main/configs/COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\n",
    "# Need at least 6GB of VRAM\n",
    "base_model = \"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"\n",
    "\n",
    "# Setup configuration with all datasets\n",
    "out_dir = \"train_outputs\"\n",
    "cfg = setup_cfg(\n",
    "    base_model,\n",
    "    all_train_datasets,  # Use all registered train datasets\n",
    "    all_val_datasets,    # Use all registered validation datasets\n",
    "    workers=4,\n",
    "    eval_period=100,\n",
    "    max_iter=3000,\n",
    "    out_dir=out_dir      # Default is \"train_outputs\"\n",
    ")\n",
    "\n",
    "# Check evaluation datasets configuration\n",
    "print(\"Test datasets:\", cfg.DATASETS.TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5e3d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = MyTrainer(cfg, patience = 5)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca990b9c",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Based on metrics.json from train_output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bb3788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from detectree2.models.train import load_json_arr\n",
    "\n",
    "experiment_metrics = load_json_arr('train_outputs/metrics.json')\n",
    "\n",
    "plt.plot(\n",
    "   [x['iteration'] for x in experiment_metrics if 'validation_loss' in x],\n",
    "   [x['validation_loss'] for x in experiment_metrics if 'validation_loss' in x], label='Total Validation Loss', color='red')\n",
    "plt.plot(\n",
    "   [x['iteration'] for x in experiment_metrics if 'total_loss' in x],\n",
    "   [x['total_loss'] for x in experiment_metrics if 'total_loss' in x], label='Total Training Loss')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Comparison of the training and validation loss of detectree2')\n",
    "plt.ylabel('Total Loss')\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
